{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419f74b5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Install\n",
    "!pip install scikit-surprise --no-deps -q\n",
    "!pip install scipy numpy scikit-learn -q\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, glob, pickle, time\n",
    "from scipy import sparse\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from surprise import SVD, Dataset, Reader, accuracy\n",
    "from surprise.model_selection import train_test_split\n",
    "print(\"âœ… All imports successful!\")\n",
    "\n",
    "\n",
    "print(\" FIXED Training (Anti-Overfitting)\")\n",
    "\n",
    "\n",
    "# Clone data\n",
    "if not os.path.exists('data'):\n",
    "    !git clone https://github.com/SimpliSolve/ai-innovation-talent-hunt-dataset.git data\n",
    "\n",
    "ML = 'data/ml-latest-small'\n",
    "UR = 'data/user_reviews'\n",
    "DC = 'data/daily_csvs'\n",
    "\n",
    "# Load MovieLens\n",
    "print(\"\\n Loading MovieLens...\")\n",
    "ratings_df = pd.read_csv(f\"{ML}/ratings.csv\")\n",
    "movies_df = pd.read_csv(f\"{ML}/movies.csv\")\n",
    "links_df = pd.read_csv(f\"{ML}/links.csv\")\n",
    "print(f\"  {len(ratings_df):,} ratings\")\n",
    "\n",
    "# Load metadata (quick)\n",
    "print(\"\\n Loading metadata...\")\n",
    "movie_metadata = {}\n",
    "for f in tqdm(glob.glob(f\"{DC}/*.csv\"), desc=\"Loading\"):\n",
    "    try:\n",
    "        df = pd.read_csv(f, low_memory=False)\n",
    "        for _,r in df.iterrows():\n",
    "            l = r.get('movie_link','')\n",
    "            if pd.notna(l) and l and l not in movie_metadata:\n",
    "                movie_metadata[l] = {'name':r.get('movie_name',''), 'genre':r.get('genre','[]')}\n",
    "    except: pass\n",
    "print(f\" {len(movie_metadata):,} movies\")\n",
    "\n",
    "# IMDB profiles\n",
    "print(\"\\n Building IMDB profiles...\")\n",
    "imdb_user_profiles = defaultdict(lambda: {'movies_reviewed':[], 'ratings':[], 'avg_rating':0, 'review_count':0})\n",
    "total = 0\n",
    "folders = sorted(os.listdir(UR)) if os.path.exists(UR) else []\n",
    "for mf in tqdm(folders, desc=\"Processing\"):\n",
    "    fp = os.path.join(UR, mf)\n",
    "    if not os.path.isdir(fp): continue\n",
    "    ml = f\"https://www.imdb.com/title/{mf}/\"\n",
    "    for cf in glob.glob(os.path.join(fp,'*.csv')):\n",
    "        try:\n",
    "            df = pd.read_csv(cf, encoding='utf-8', on_bad_lines='skip')\n",
    "            for _,r in df.iterrows():\n",
    "                uid = str(r.get('User_id','')).strip()\n",
    "                rs = str(r.get('User_rating','')).strip()\n",
    "                if not uid or not rs or uid=='nan': continue\n",
    "                try: rt = float(rs.split('/')[0]) if '/' in rs else float(rs)\n",
    "                except: continue\n",
    "                imdb_user_profiles[uid]['movies_reviewed'].append(ml)\n",
    "                imdb_user_profiles[uid]['ratings'].append(rt)\n",
    "                imdb_user_profiles[uid]['review_count'] += 1\n",
    "                total += 1\n",
    "        except: pass\n",
    "for u,p in imdb_user_profiles.items():\n",
    "    if p['ratings']: p['avg_rating'] = np.mean(p['ratings'])\n",
    "print(f\" {total:,} reviews from {len(imdb_user_profiles):,} users\")\n",
    "\n",
    "# Features (same as before)\n",
    "print(\"\\n Creating features...\")\n",
    "all_genres = set()\n",
    "for g in movies_df['genres']:\n",
    "    if g != '(no genres listed)': all_genres.update(g.split('|'))\n",
    "genres_list = sorted(list(all_genres))\n",
    "n_genres = len(genres_list)\n",
    "genre_to_idx = {g:i for i,g in enumerate(genres_list)}\n",
    "\n",
    "users = sorted(ratings_df['userId'].unique())\n",
    "items = sorted(ratings_df['movieId'].unique())\n",
    "n_users, n_items = len(users), len(items)\n",
    "user_to_idx = {u:i for i,u in enumerate(users)}\n",
    "idx_to_user = {i:u for u,i in user_to_idx.items()}\n",
    "item_to_idx = {m:i for i,m in enumerate(items)}\n",
    "idx_to_item = {i:m for m,i in item_to_idx.items()}\n",
    "\n",
    "imdb_to_ml, ml_to_imdb = {}, {}\n",
    "for _,r in links_df.iterrows():\n",
    "    mid, iid = int(r['movieId']), str(int(r['imdbId'])).zfill(7)\n",
    "    l = f\"https://www.imdb.com/title/tt{iid}/\"\n",
    "    imdb_to_ml[l] = mid; imdb_to_ml[l.rstrip('/')] = mid; imdb_to_ml[f\"tt{iid}\"] = mid\n",
    "    ml_to_imdb[mid] = l\n",
    "\n",
    "movie_genres_dict = {}\n",
    "for _,r in movies_df.iterrows():\n",
    "    gv = np.zeros(n_genres)\n",
    "    if r['genres'] != '(no genres listed)':\n",
    "        for g in r['genres'].split('|'):\n",
    "            if g in genre_to_idx: gv[genre_to_idx[g]] = 1.0\n",
    "    movie_genres_dict[r['movieId']] = gv\n",
    "\n",
    "ir,ic,id_ = [],[],[]\n",
    "for _,r in movies_df.iterrows():\n",
    "    if r['movieId'] not in item_to_idx: continue\n",
    "    idx = item_to_idx[r['movieId']]\n",
    "    if r['genres'] != '(no genres listed)':\n",
    "        for g in r['genres'].split('|'):\n",
    "            if g in genre_to_idx: ir.append(idx); ic.append(genre_to_idx[g]); id_.append(1.0)\n",
    "item_features = sparse.csr_matrix((id_,(ir,ic)), shape=(n_items,n_genres))\n",
    "\n",
    "ugp = np.zeros((n_users,n_genres)); ugc = np.zeros((n_users,n_genres))\n",
    "for _,r in ratings_df.iterrows():\n",
    "    if r['userId'] in user_to_idx and r['movieId'] in movie_genres_dict:\n",
    "        ui = user_to_idx[r['userId']]\n",
    "        ugp[ui] += movie_genres_dict[r['movieId']] * (r['rating']/5.0)\n",
    "        ugc[ui] += movie_genres_dict[r['movieId']]\n",
    "with np.errstate(divide='ignore',invalid='ignore'):\n",
    "    user_features_dense = np.where(ugc>0, ugp/ugc, 0)\n",
    "\n",
    "rc = ratings_df.groupby('movieId').size()\n",
    "ar = ratings_df.groupby('movieId')['rating'].mean()\n",
    "mc = rc.max()\n",
    "movie_popularity = {}\n",
    "for mid in item_to_idx.keys():\n",
    "    c,a = rc.get(mid,0), ar.get(mid,3.0)\n",
    "    movie_popularity[mid] = {'count':int(c), 'avg_rating':float(a), 'popularity_score':float((c/mc)*(a/5.0)) if mc>0 else 0}\n",
    "top_popular_movies = sorted(movie_popularity.keys(), key=lambda x: movie_popularity[x]['popularity_score'], reverse=True)[:100]\n",
    "print(f\" {n_users} users, {n_items} items, {n_genres} genres\")\n",
    "\n",
    "# ðŸ”§ FIXED: Anti-Overfitting Training\n",
    "print(\"\\nðŸš€ Training SVD with ANTI-OVERFITTING settings...\")\n",
    "reader = Reader(rating_scale=(0.5, 5.0))\n",
    "sdata = Dataset.load_from_df(ratings_df[['userId','movieId','rating']], reader)\n",
    "trainset, testset = train_test_split(sdata, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"\\n FIXED Hyperparameters (Anti-Overfitting):\")\n",
    "print(\"   OLD: n_factors=150, reg_all=0.02, lr_all=0.005\")\n",
    "print(\"   NEW: n_factors=100, reg_all=0.2, lr_all=0.003\")\n",
    "print(\"   NEW: Early stopping based on validation gap\")\n",
    "\n",
    "# Track with early stopping\n",
    "eps = [5,10,15,20,25,30]\n",
    "tr_h, te_h = [], []\n",
    "best_gap = float('inf')\n",
    "best_epoch = 0\n",
    "best_model = None\n",
    "\n",
    "print(\"\\n Training with early stopping:\")\n",
    "for e in eps:\n",
    "    # FIXED: Lower complexity, higher regularization\n",
    "    m = SVD(n_factors=100, n_epochs=e, lr_all=0.003, reg_all=0.2, random_state=42)\n",
    "    m.fit(trainset)\n",
    "    tr = accuracy.rmse(m.test(trainset.build_testset()), verbose=False)\n",
    "    te = accuracy.rmse(m.test(testset), verbose=False)\n",
    "    gap = te - tr\n",
    "    tr_h.append(tr); te_h.append(te)\n",
    "\n",
    "    if gap < best_gap:\n",
    "        best_gap = gap\n",
    "        best_epoch = e\n",
    "        best_model = m\n",
    "\n",
    "    status = \"âœ…\" if gap < 0.15 else (\"âš ï¸\" if gap < 0.3 else \"âŒ\")\n",
    "    print(f\"  Epoch {e:2d}: Train={tr:.4f}, Test={te:.4f}, Gap={gap:.4f} {status}\")\n",
    "\n",
    "    # Early stop if gap is good\n",
    "    if gap < 0.1:\n",
    "        print(f\"   Early stopping at epoch {e} (gap < 0.1)\")\n",
    "        break\n",
    "\n",
    "print(f\"\\n Best epoch: {best_epoch} (gap: {best_gap:.4f})\")\n",
    "\n",
    "# Use best epoch for final model\n",
    "bp = {'n_factors':100, 'n_epochs':best_epoch, 'lr_all':0.003, 'reg_all':0.2}\n",
    "final_model = SVD(**bp, random_state=42)\n",
    "final_model.fit(sdata.build_full_trainset())\n",
    "\n",
    "# Evaluate\n",
    "eval_model = SVD(**bp, random_state=42)\n",
    "eval_model.fit(trainset)\n",
    "tp = eval_model.test(testset)\n",
    "rmse = accuracy.rmse(tp, verbose=False)\n",
    "mae = accuracy.mae(tp, verbose=False)\n",
    "\n",
    "# Recall\n",
    "print(\"\\n Recall@K...\")\n",
    "def recall_k(preds, k=5, th=3.5):\n",
    "    ud = defaultdict(list)\n",
    "    for p in preds: ud[p.uid].append((p.est, p.r_ui))\n",
    "    recs = []\n",
    "    for _,rats in ud.items():\n",
    "        rats.sort(key=lambda x:x[0], reverse=True)\n",
    "        nr = sum(t>=th for _,t in rats)\n",
    "        nrr = sum((t>=th)and(e>=th) for e,t in rats[:k])\n",
    "        if nr>0: recs.append(nrr/min(nr,k))\n",
    "    return np.mean(recs) if recs else 0\n",
    "\n",
    "rr = {}\n",
    "for k in [1,3,5,10]:\n",
    "    rr[k] = recall_k(tp, k=k)\n",
    "    print(f\"  Recall@{k}: {rr[k]:.4f}\")\n",
    "\n",
    "# Plots\n",
    "print(\"\\n Creating plots...\")\n",
    "fig = plt.figure(figsize=(14,10))\n",
    "\n",
    "ax1 = fig.add_subplot(2,2,1)\n",
    "ax1.plot(eps[:len(tr_h)], tr_h, 'b-o', label='Train', lw=2)\n",
    "ax1.plot(eps[:len(te_h)], te_h, 'r-s', label='Test', lw=2)\n",
    "ax1.fill_between(eps[:len(tr_h)], tr_h, te_h, alpha=0.3, color='yellow')\n",
    "if best_epoch > 0:\n",
    "    ax1.axvline(best_epoch, color='green', linestyle='--', linewidth=2, label=f'Best Epoch {best_epoch}')\n",
    "ax1.set_xlabel('Epochs'); ax1.set_ylabel('RMSE')\n",
    "ax1.set_title(' Overfitting Detection (FIXED)', fontweight='bold')\n",
    "ax1.legend(); ax1.grid(True, alpha=0.3)\n",
    "gap = te_h[-1]-tr_h[-1]\n",
    "st = \"GOOD\" if gap<0.1 else (\"âš ï¸MODERATE\" if gap<0.2 else \"âŒOVERFIT\")\n",
    "ax1.annotate(f'{st}\\nGap:{gap:.4f}', xy=(0.7,0.8), xycoords='axes fraction', fontweight='bold')\n",
    "\n",
    "ax2 = fig.add_subplot(2,2,2)\n",
    "ax2.bar(range(len(rr)), list(rr.values()), color='#2ecc71')\n",
    "ax2.set_xticks(range(len(rr))); ax2.set_xticklabels([f'@{k}' for k in rr.keys()])\n",
    "ax2.set_title(' Recall@K', fontweight='bold'); ax2.set_ylim(0,1)\n",
    "\n",
    "ax3 = fig.add_subplot(2,2,3)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "act = [p.r_ui for p in tp]; pred = [p.est for p in tp]\n",
    "bins = [0.5,1.5,2.5,3.5,4.5,5.5]\n",
    "cm = confusion_matrix(np.digitize(act,bins), np.digitize(pred,bins), labels=[1,2,3,4,5])\n",
    "cmn = cm.astype('float')/cm.sum(axis=1)[:,np.newaxis]\n",
    "sns.heatmap(cmn, annot=True, fmt='.2f', cmap='Blues', ax=ax3, xticklabels=[1,2,3,4,5], yticklabels=[1,2,3,4,5])\n",
    "ax3.set_xlabel('Predicted'); ax3.set_ylabel('Actual')\n",
    "ax3.set_title(' Confusion Matrix', fontweight='bold')\n",
    "\n",
    "ax4 = fig.add_subplot(2,2,4)\n",
    "ax4.hist([p.est-p.r_ui for p in tp], bins=50, color='#9b59b6', alpha=0.7)\n",
    "ax4.axvline(0, color='red', ls='--', lw=2)\n",
    "ax4.set_xlabel('Error'); ax4.set_title(' Error Distribution', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_metrics_fixed.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# Results\n",
    "\n",
    "print(f\"  RMSE: {rmse:.4f} | MAE: {mae:.4f} | Recall@5: {rr[5]:.4f}\")\n",
    "print(f\"  Status: {st} (Gap: {gap:.4f})\")\n",
    "print(f\"  Best Epoch: {best_epoch}\")\n",
    "\n",
    "\n",
    "# Save\n",
    "print(\"\\n Saving model...\")\n",
    "os.makedirs('Resources', exist_ok=True)\n",
    "idict = {k:dict(v) for k,v in imdb_user_profiles.items()}\n",
    "md = {\n",
    "    'svd_model':final_model, 'model_type':'svd_hybrid_fixed', 'hyperparams':bp,\n",
    "    'n_users':n_users, 'n_items':n_items, 'n_genres':n_genres,\n",
    "    'user_to_idx':user_to_idx, 'idx_to_user':idx_to_user,\n",
    "    'item_to_idx':item_to_idx, 'idx_to_item':idx_to_item,\n",
    "    'imdb_to_ml':imdb_to_ml, 'ml_to_imdb':ml_to_imdb,\n",
    "    'genres':genres_list, 'genre_to_idx':genre_to_idx,\n",
    "    'item_genre_matrix':item_features.toarray(),\n",
    "    'user_genre_prefs':user_features_dense,\n",
    "    'movie_genres_dict':movie_genres_dict,\n",
    "    'movie_popularity':movie_popularity,\n",
    "    'top_popular_movies':top_popular_movies,\n",
    "    'imdb_user_profiles':idict,\n",
    "    'movie_metadata':movie_metadata,\n",
    "    'movie_names':dict(zip(movies_df['movieId'], movies_df['title'])),\n",
    "    'performance':{'rmse':rmse,'mae':mae,'recall@5':rr[5],'gap':gap}\n",
    "}\n",
    "with open('Resources/hybrid_model.pkl','wb') as f: pickle.dump(md,f)\n",
    "print(f\" Saved: {os.path.getsize('Resources/hybrid_model.pkl')/(1024*1024):.2f} MB\")\n",
    "\n",
    "# Download\n",
    "from google.colab import files\n",
    "files.download('Resources/hybrid_model.pkl')\n",
    "files.download('training_metrics_fixed.png')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
