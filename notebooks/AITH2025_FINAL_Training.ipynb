{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21adf609",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üì¶ Installing required packages...\\n\")\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"uninstall\", \"-y\", \"numpy\"])\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"numpy==1.24.3\"])\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"pandas==2.0.3\"])\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"scikit-learn==1.3.2\"])\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"scipy==1.11.4\"])\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"--no-deps\", \"scikit-surprise==1.1.3\"])\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"matplotlib\", \"seaborn\"])\n",
    "\n",
    "print(\"‚úÖ Packages installed successfully!\")\n",
    "\n",
    "import numpy as np\n",
    "print(f\"\\nüìä NumPy version: {np.__version__}\")\n",
    "assert np.__version__.startswith('1.'), f\"NumPy version {np.__version__} may cause issues. Expected 1.x\"\n",
    "print(\"‚úÖ NumPy compatibility verified!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d909ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "uploaded = files.upload()\n",
    "\n",
    "zip_filename = list(uploaded.keys())[0]\n",
    "print(f\"\\nüìÇ Extracting {zip_filename}...\")\n",
    "\n",
    "with zipfile.ZipFile(zip_filename, 'r') as zip_ref:\n",
    "    zip_ref.extractall('data')\n",
    "\n",
    "print(\"‚úÖ Dataset extracted successfully!\")\n",
    "\n",
    "for root, dirs, files in os.walk('data'):\n",
    "    if 'ratings.csv' in files:\n",
    "        dataset_path = root\n",
    "        print(f\"‚úÖ Found dataset at: {dataset_path}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba728217",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from surprise import Dataset, Reader\n",
    "\n",
    "print(\"üìä Loading MovieLens dataset...\\n\")\n",
    "\n",
    "ratings_df = pd.read_csv(os.path.join(dataset_path, 'ratings.csv'))\n",
    "print(f\"‚úÖ Loaded {len(ratings_df):,} ratings\")\n",
    "print(f\"   - Users: {ratings_df['userId'].nunique():,}\")\n",
    "print(f\"   - Movies: {ratings_df['movieId'].nunique():,}\")\n",
    "print(f\"   - Rating range: {ratings_df['rating'].min():.1f} - {ratings_df['rating'].max():.1f}\")\n",
    "print(f\"   - Average rating: {ratings_df['rating'].mean():.2f}\")\n",
    "\n",
    "reader = Reader(rating_scale=(0.5, 5.0))\n",
    "data = Dataset.load_from_df(ratings_df[['userId', 'movieId', 'rating']], reader)\n",
    "\n",
    "print(\"\\n‚úÖ Dataset prepared for training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d4734c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import SVD\n",
    "from surprise.model_selection import cross_validate\n",
    "import time\n",
    "\n",
    "print(\"üîç Testing hyperparameter configurations...\\n\")\n",
    "\n",
    "configs = [\n",
    "    {'n_factors': 100, 'n_epochs': 30, 'lr_all': 0.005, 'reg_all': 0.02, 'name': 'Balanced'},\n",
    "    {'n_factors': 150, 'n_epochs': 25, 'lr_all': 0.007, 'reg_all': 0.015, 'name': 'High Factors'},\n",
    "    {'n_factors': 200, 'n_epochs': 20, 'lr_all': 0.01, 'reg_all': 0.01, 'name': 'Deep Learning'},\n",
    "]\n",
    "\n",
    "best_config = None\n",
    "best_rmse = float('inf')\n",
    "\n",
    "for config in configs:\n",
    "    print(f\"Testing {config['name']} configuration...\")\n",
    "    \n",
    "    model = SVD(\n",
    "        n_factors=config['n_factors'],\n",
    "        n_epochs=config['n_epochs'],\n",
    "        lr_all=config['lr_all'],\n",
    "        reg_all=config['reg_all'],\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    cv_results = cross_validate(model, data, measures=['RMSE', 'MAE'], cv=3, verbose=False)\n",
    "    \n",
    "    avg_rmse = cv_results['test_rmse'].mean()\n",
    "    avg_mae = cv_results['test_mae'].mean()\n",
    "    \n",
    "    print(f\"  RMSE: {avg_rmse:.4f} | MAE: {avg_mae:.4f}\")\n",
    "    \n",
    "    if avg_rmse < best_rmse:\n",
    "        best_rmse = avg_rmse\n",
    "        best_config = config\n",
    "\n",
    "print(f\"\\nüèÜ Best configuration: {best_config['name']}\")\n",
    "print(f\"   RMSE: {best_rmse:.4f}\")\n",
    "print(f\"   Factors: {best_config['n_factors']}, Epochs: {best_config['n_epochs']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3e31d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import SVD\n",
    "from surprise.model_selection import train_test_split\n",
    "\n",
    "print(\"üöÄ Training final model with optimized hyperparameters...\\n\")\n",
    "\n",
    "trainset_full = data.build_full_trainset()\n",
    "trainset, testset = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "final_model = SVD(\n",
    "    n_factors=best_config['n_factors'],\n",
    "    n_epochs=best_config['n_epochs'],\n",
    "    lr_all=best_config['lr_all'],\n",
    "    reg_all=best_config['reg_all'],\n",
    "    random_state=42,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "final_model.fit(trainset)\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(f\"\\n‚úÖ Training completed in {training_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391f3bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import accuracy\n",
    "\n",
    "print(\"üìà Evaluating model on test set...\\n\")\n",
    "\n",
    "predictions = final_model.test(testset)\n",
    "\n",
    "rmse = accuracy.rmse(predictions, verbose=False)\n",
    "mae = accuracy.mae(predictions, verbose=False)\n",
    "\n",
    "print(f\"Test Set Performance:\")\n",
    "print(f\"  RMSE: {rmse:.4f}\")\n",
    "print(f\"  MAE:  {mae:.4f}\")\n",
    "\n",
    "exact_matches = sum(1 for pred in predictions if abs(pred.est - pred.r_ui) < 0.1)\n",
    "within_half_star = sum(1 for pred in predictions if abs(pred.est - pred.r_ui) <= 0.5)\n",
    "within_one_star = sum(1 for pred in predictions if abs(pred.est - pred.r_ui) <= 1.0)\n",
    "\n",
    "total = len(predictions)\n",
    "print(f\"\\nPrediction Accuracy:\")\n",
    "print(f\"  Exact match (¬±0.1): {exact_matches/total*100:.2f}%\")\n",
    "print(f\"  Within ¬±0.5 stars:  {within_half_star/total*100:.2f}%\")\n",
    "print(f\"  Within ¬±1.0 stars:  {within_one_star/total*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2daf3e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "print(\"üéØ Calculating Recall@K metrics...\\n\")\n",
    "\n",
    "def get_top_n(predictions, n=10):\n",
    "    top_n = defaultdict(list)\n",
    "    for uid, iid, true_r, est, _ in predictions:\n",
    "        top_n[uid].append((iid, est))\n",
    "    \n",
    "    for uid, user_ratings in top_n.items():\n",
    "        user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_n[uid] = user_ratings[:n]\n",
    "    \n",
    "    return top_n\n",
    "\n",
    "def precision_recall_at_k(predictions, k=10, threshold=3.5):\n",
    "    user_est_true = defaultdict(list)\n",
    "    for uid, iid, true_r, est, _ in predictions:\n",
    "        user_est_true[uid].append((iid, est, true_r))\n",
    "    \n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    \n",
    "    for uid, user_ratings in user_est_true.items():\n",
    "        user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        n_rel = sum((true_r >= threshold) for (_, _, true_r) in user_ratings)\n",
    "        n_rec_k = sum((est >= threshold) for (_, est, _) in user_ratings[:k])\n",
    "        n_rel_and_rec_k = sum(\n",
    "            ((true_r >= threshold) and (est >= threshold))\n",
    "            for (_, est, true_r) in user_ratings[:k]\n",
    "        )\n",
    "        \n",
    "        precisions.append(n_rel_and_rec_k / n_rec_k if n_rec_k != 0 else 0)\n",
    "        recalls.append(n_rel_and_rec_k / n_rel if n_rel != 0 else 0)\n",
    "    \n",
    "    return sum(precisions) / len(precisions), sum(recalls) / len(recalls)\n",
    "\n",
    "recall_metrics = {}\n",
    "for k in [1, 3, 5]:\n",
    "    precision, recall = precision_recall_at_k(predictions, k=k, threshold=3.5)\n",
    "    recall_metrics[f'Recall@{k}'] = recall\n",
    "    print(f\"Recall@{k}: {recall:.4f} | Precision@{k}: {precision:.4f}\")\n",
    "\n",
    "print(f\"\\nüèÜ Competition Metrics:\")\n",
    "print(f\"   Recall@5: {recall_metrics['Recall@5']:.4f}\")\n",
    "print(f\"   Recall@3: {recall_metrics['Recall@3']:.4f}\")\n",
    "print(f\"   Recall@1: {recall_metrics['Recall@1']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620ae37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîÑ Retraining on full dataset for final submission...\\n\")\n",
    "\n",
    "trainset_full = data.build_full_trainset()\n",
    "\n",
    "production_model = SVD(\n",
    "    n_factors=best_config['n_factors'],\n",
    "    n_epochs=best_config['n_epochs'],\n",
    "    lr_all=best_config['lr_all'],\n",
    "    reg_all=best_config['reg_all'],\n",
    "    random_state=42,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "production_model.fit(trainset_full)\n",
    "\n",
    "print(\"\\n‚úÖ Production model trained on full dataset!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1873739",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"üìä Creating performance visualizations...\\n\")\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (15, 10)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "errors = [pred.est - pred.r_ui for pred in predictions]\n",
    "axes[0, 0].hist(errors, bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0, 0].set_title('Prediction Error Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Error (Predicted - Actual)')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].axvline(x=0, color='red', linestyle='--', linewidth=2, label='Perfect Prediction')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "actual = [pred.r_ui for pred in predictions[:1000]]\n",
    "predicted = [pred.est for pred in predictions[:1000]]\n",
    "axes[0, 1].scatter(actual, predicted, alpha=0.3, s=10)\n",
    "axes[0, 1].plot([0, 5], [0, 5], 'r--', linewidth=2, label='Perfect Prediction')\n",
    "axes[0, 1].set_title('Actual vs Predicted Ratings', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Actual Rating')\n",
    "axes[0, 1].set_ylabel('Predicted Rating')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "k_values = [1, 3, 5]\n",
    "recall_values = [recall_metrics[f'Recall@{k}'] for k in k_values]\n",
    "bars = axes[1, 0].bar([str(k) for k in k_values], recall_values, color=['#FF6B6B', '#4ECDC4', '#45B7D1'], edgecolor='black')\n",
    "axes[1, 0].set_title('Recall@K Performance', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('K')\n",
    "axes[1, 0].set_ylabel('Recall Score')\n",
    "axes[1, 0].set_ylim([0, 1])\n",
    "for bar, value in zip(bars, recall_values):\n",
    "    height = bar.get_height()\n",
    "    axes[1, 0].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                    f'{value:.4f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "axes[1, 1].axis('off')\n",
    "summary_text = f\"\"\"\n",
    "Model Performance Summary\n",
    "{'='*40}\n",
    "\n",
    "Configuration: {best_config['name']}\n",
    "  ‚Ä¢ Factors: {best_config['n_factors']}\n",
    "  ‚Ä¢ Epochs: {best_config['n_epochs']}\n",
    "  ‚Ä¢ Learning Rate: {best_config['lr_all']}\n",
    "  ‚Ä¢ Regularization: {best_config['reg_all']}\n",
    "\n",
    "Error Metrics:\n",
    "  ‚Ä¢ RMSE: {rmse:.4f}\n",
    "  ‚Ä¢ MAE: {mae:.4f}\n",
    "\n",
    "Competition Metrics:\n",
    "  ‚Ä¢ Recall@5: {recall_metrics['Recall@5']:.4f}\n",
    "  ‚Ä¢ Recall@3: {recall_metrics['Recall@3']:.4f}\n",
    "  ‚Ä¢ Recall@1: {recall_metrics['Recall@1']:.4f}\n",
    "\n",
    "Accuracy:\n",
    "  ‚Ä¢ Within ¬±0.5 stars: {within_half_star/total*100:.1f}%\n",
    "  ‚Ä¢ Within ¬±1.0 stars: {within_one_star/total*100:.1f}%\n",
    "\n",
    "Training Time: {training_time:.2f}s\n",
    "\"\"\"\n",
    "axes[1, 1].text(0.1, 0.5, summary_text, fontsize=11, family='monospace',\n",
    "                verticalalignment='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('model_performance.png', dpi=300, bbox_inches='tight')\n",
    "print(\"‚úÖ Visualization saved as 'model_performance.png'\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905ece08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json\n",
    "\n",
    "print(\"üíæ Saving trained model...\\n\")\n",
    "\n",
    "model_package = {\n",
    "    'svd_model': production_model,\n",
    "    'trainset': trainset_full,\n",
    "    'metadata': {\n",
    "        'model_type': 'SVD Collaborative Filtering',\n",
    "        'library': 'scikit-surprise 1.1.3',\n",
    "        'training_date': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'dataset': 'MovieLens latest-small',\n",
    "        'n_ratings': len(ratings_df),\n",
    "        'n_users': ratings_df['userId'].nunique(),\n",
    "        'n_movies': ratings_df['movieId'].nunique(),\n",
    "        'hyperparameters': {\n",
    "            'n_factors': best_config['n_factors'],\n",
    "            'n_epochs': best_config['n_epochs'],\n",
    "            'lr_all': best_config['lr_all'],\n",
    "            'reg_all': best_config['reg_all'],\n",
    "        },\n",
    "        'performance': {\n",
    "            'rmse': float(rmse),\n",
    "            'mae': float(mae),\n",
    "            'recall_at_5': float(recall_metrics['Recall@5']),\n",
    "            'recall_at_3': float(recall_metrics['Recall@3']),\n",
    "            'recall_at_1': float(recall_metrics['Recall@1']),\n",
    "            'accuracy_within_0.5': float(within_half_star/total),\n",
    "            'accuracy_within_1.0': float(within_one_star/total),\n",
    "        },\n",
    "        'training_time_seconds': float(training_time),\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('recommendation_model.pkl', 'wb') as f:\n",
    "    pickle.dump(model_package, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('model_metadata.json', 'w') as f:\n",
    "    json.dump(model_package['metadata'], f, indent=2)\n",
    "\n",
    "print(\"‚úÖ Model saved successfully!\")\n",
    "print(\"\\nSaved files:\")\n",
    "print(\"  - recommendation_model.pkl (trained model)\")\n",
    "print(\"  - model_metadata.json (performance metrics)\")\n",
    "print(\"  - model_performance.png (visualizations)\")\n",
    "\n",
    "import os\n",
    "model_size = os.path.getsize('recommendation_model.pkl') / (1024 * 1024)\n",
    "print(f\"\\nModel size: {model_size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025c8c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "print(\"üì• Downloading files to your computer...\\n\")\n",
    "\n",
    "print(\"Downloading recommendation_model.pkl...\")\n",
    "files.download('recommendation_model.pkl')\n",
    "\n",
    "print(\"Downloading model_metadata.json...\")\n",
    "files.download('model_metadata.json')\n",
    "\n",
    "print(\"Downloading model_performance.png...\")\n",
    "files.download('model_performance.png')\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéâ TRAINING COMPLETE! üéâ\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nNext Steps:\")\n",
    "print(\"1. Move 'recommendation_model.pkl' to your project's 'models/' folder\")\n",
    "print(\"2. Test inference: python inference.py --test_data_path sample_test_phase_1\")\n",
    "print(\"3. Verify output files are generated in 'output/' folder\")\n",
    "print(\"4. Include 'model_performance.png' in your technical report\")\n",
    "print(\"5. Use metrics from 'model_metadata.json' for your report\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"Model Performance (for your report):\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Recall@5: {recall_metrics['Recall@5']:.4f}\")\n",
    "print(f\"Recall@3: {recall_metrics['Recall@3']:.4f}\")\n",
    "print(f\"Recall@1: {recall_metrics['Recall@1']:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"MAE: {mae:.4f}\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
